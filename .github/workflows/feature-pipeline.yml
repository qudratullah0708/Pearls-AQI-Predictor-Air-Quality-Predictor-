name: Hourly AQI Data Collection

on:
  schedule:
    # Run every hour at minute 0
    - cron: '0 * * * *'
  workflow_dispatch:  # Allow manual trigger
  push:
    branches: [ main ]
      paths:
      - 'feature_pipeline.py'
      - 'config.py'

jobs:
  collect-aqi-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create service account key file
      run: |
        echo '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}' > aqi-service-account.json
        echo "GCP_SERVICE_ACCOUNT_KEY_PATH=aqi-service-account.json" >> .env
        echo "AQICN_TOKEN=${{ secrets.AQICN_TOKEN }}" >> .env
        echo "GCP_PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}" >> .env
        echo "GCP_REGION=us-central1" >> .env
        echo "BIGQUERY_DATASET_ID=aqi_dataset" >> .env
        echo "BIGQUERY_TABLE_ID=aqi_features" >> .env
        
    - name: Run AQI Feature Pipeline
      run: |
        echo "🚀 Starting automated AQI data collection..."
        echo "⏰ Timestamp: $(date)"
        python feature_pipeline.py
        
    - name: Clean up service account key
      if: always()
      run: |
        rm -f aqi-service-account.json
        rm -f .env
        
    - name: Upload logs on failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: feature-pipeline-logs-${{ github.run_number }}
        path: ./
        retention-days: 7
